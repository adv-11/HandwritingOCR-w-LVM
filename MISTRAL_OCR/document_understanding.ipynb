{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1EBW_a6gRUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6646d9c-1c77-46ba-ffc3-0425f715b6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai\n",
            "  Downloading mistralai-1.5.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Collecting jsonpath-python>=1.0.6 (from mistralai)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.8.2)\n",
            "Collecting typing-inspect>=0.9.0 (from mistralai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n",
            "Downloading mistralai-1.5.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, jsonpath-python, eval-type-backport, typing-inspect, mistralai\n",
            "Successfully installed eval-type-backport-0.2.2 jsonpath-python-1.0.6 mistralai-1.5.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AwG2kwfTlbW1"
      },
      "outputs": [],
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"api\"\n",
        "client = Mistral(api_key=api_key)\n",
        "text_model = \"mistral-small-latest\"\n",
        "ocr_model = \"mistral-ocr-latest\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F35KDRN-nEMv"
      },
      "source": [
        "### System and Tool\n",
        "For the model to be aware of its purpose and what it can do, it's important to provide a clear system prompt with instructions and explanations of any tools it may have access to.\n",
        "\n",
        "Let's define a system prompt and the tools it will have access to, in this case, `open_urls`.\n",
        "\n",
        "*Note: `open_urls` can easily be customized with other resources and models ( for summarization, for example ) and many other features. In this demo, we are going for a simpler approach.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zzgxk6qTgGU9"
      },
      "outputs": [],
      "source": [
        "system = \"\"\"You are an AI Assistant with document understanding via URLs. You will be provided with URLs, and you must answer any questions related to those documents.\n",
        "\n",
        "# OPEN URLS INSTRUCTIONS\n",
        "You can open URLs by using the `open_urls` tool. It will open webpages and apply OCR to them, retrieving the contents. Use those contents to answer the user.\n",
        "Only URLs pointing to PDFs and images are supported; you may encounter an error if they are not; provide that information to the user if required.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _perform_ocr(url: str) -> str:\n",
        "    try:   # Apply OCR to the PDF URL\n",
        "        response = client.ocr.process(\n",
        "            model=ocr_model,\n",
        "            document={\n",
        "                \"type\": \"document_url\",\n",
        "                \"document_url\": url\n",
        "                }\n",
        "            )\n",
        "    except Exception:\n",
        "        try:  # IF PDF OCR fails, try Image OCR\n",
        "            response = client.ocr.process(\n",
        "                model=ocr_model,\n",
        "                document={\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": url\n",
        "                    }\n",
        "                )\n",
        "        except Exception as e:\n",
        "            return e  # Return the error to the model if it fails, otherwise return the contents\n",
        "    return \"\\n\\n\".join([f\"### Page {i+1}\\n{response.pages[i].markdown}\" for i in range(len(response.pages))])"
      ],
      "metadata": {
        "id": "SxP7DlEHWXnK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_urls(urls: list) -> str:\n",
        "    contents = \"# Documents\"\n",
        "    for url in urls:\n",
        "        contents += f\"\\n\\n## URL: {url}\\n{_perform_ocr(url)}\"\n",
        "    return contents"
      ],
      "metadata": {
        "id": "s9PgX9fqWY1m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BagY4xg0nSSg"
      },
      "source": [
        "We also have to define the Tool Schema that will be provided to our API and model.\n",
        "\n",
        "By following the [documentation](https://docs.mistral.ai/capabilities/function_calling/), we can create something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hpBKzNOfliQr"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"open_urls\",\n",
        "            \"description\": \"Open URLs websites (PDFs and Images) and perform OCR on them.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"urls\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"The URLs list.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"urls\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names_to_functions = {\n",
        "    'open_urls': open_urls\n",
        "}"
      ],
      "metadata": {
        "id": "DqalxqIWWVL1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6bE08lPngrm"
      },
      "source": [
        "### Test\n",
        "Everything is ready; we can quickly create a while loop to chat with our model directly in the console.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVeVmWn_ljRo",
        "outputId": "f02d9906-979a-42a2-a706-47bf4dfc700f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User > Hi\n",
            "Assistant > Hello! How can I assist you today?\n",
            "User > summarise\n",
            "Assistant > Sure, I can help with that. Please provide the URLs of the documents you would like me to summarize.\n",
            "User > https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/mistral7b.pdf\n",
            "Assistant > The document titled \"Mistral 7B\" introduces a 7-billion-parameter language model designed for superior performance and efficiency. Developed by a team including Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, and others, Mistral 7B outperforms other models like Llama 2 and Llama 1 in various benchmarks, particularly in reasoning, mathematics, and code generation. The model uses grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle longer sequences efficiently.\n",
            "\n",
            "### Key Points:\n",
            "\n",
            "1. **Performance and Efficiency**:\n",
            "   - Mistral 7B outperforms Llama 2 13B across all benchmarks and Llama 1 34B in specific areas like mathematics and code generation.\n",
            "   - The model is designed to be efficient, using GQA and SWA to reduce inference costs and handle longer sequences.\n",
            "\n",
            "2. **Architectural Details**:\n",
            "   - Based on a transformer architecture with specific parameters like dimension, number of layers, and attention mechanisms.\n",
            "   - Introduces sliding window attention and a rolling buffer cache to manage memory and computational efficiency.\n",
            "\n",
            "3. **Results**:\n",
            "   - Evaluated on a wide range of benchmarks including commonsense reasoning, world knowledge, reading comprehension, mathematics, and code generation.\n",
            "   - Mistral 7B shows superior performance in code, mathematics, and reasoning benchmarks compared to Llama models.\n",
            "\n",
            "4. **Instruction Fine-Tuning**:\n",
            "   - Mistral 7B - Instruct, a fine-tuned version, outperforms Llama 2 13B - Chat on both human and automated benchmarks.\n",
            "   - Demonstrates the model's adaptability and superior performance in instruction-following tasks.\n",
            "\n",
            "5. **Guardrails and Content Moderation**:\n",
            "   - Introduces system prompts to enforce guardrails and ensure the model generates safe and appropriate content.\n",
            "   - Mistral 7B - Instruct can also act as a content moderator, classifying prompts and generated answers into categories like illegal activities, hateful content, and unqualified advice.\n",
            "\n",
            "6. **Conclusion**:\n",
            "   - The work on Mistral 7B suggests that language models can compress knowledge more effectively than previously thought, opening new perspectives in the field.\n",
            "   - Acknowledges the contributions of various teams and tools that aided in the development and implementation of Mistral 7B.\n",
            "\n",
            "### References:\n",
            "The document includes a comprehensive list of references to support the claims and methodologies used in the development of Mistral 7B.\n",
            "User > quit ()\n",
            "Assistant > Goodbye! If you have more questions in the future, feel free to ask. Have a great day!\n",
            "User > quit\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": system}]\n",
        "while True:\n",
        "    # Insert user input, quit if desired\n",
        "    user_input = input(\"User > \")\n",
        "    if user_input == \"quit\":\n",
        "        break\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Loop Mistral Small tool use until no tool called\n",
        "    while True:\n",
        "        response = client.chat.complete(\n",
        "            model = text_model,\n",
        "            messages = messages,\n",
        "            temperature = 0,\n",
        "            tools = tools\n",
        "        )\n",
        "        messages.append({\"role\":\"assistant\", \"content\": response.choices[0].message.content, \"tool_calls\": response.choices[0].message.tool_calls})\n",
        "\n",
        "        # If tool called, run tool and continue, else break loop and reply\n",
        "        if response.choices[0].message.tool_calls:\n",
        "            tool_call = response.choices[0].message.tool_calls[0]\n",
        "            function_name = tool_call.function.name\n",
        "            function_params = json.loads(tool_call.function.arguments)\n",
        "            function_result = names_to_functions[function_name](**function_params)\n",
        "            messages.append({\"role\":\"tool\", \"name\":function_name, \"content\":function_result, \"tool_call_id\":tool_call.id})\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(\"Assistant >\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Built-In\n",
        "Mistral provides a built-in feature that leverages OCR with all models. By providing a URL pointing to a document, you can extract text data that will be provided to the model.\n"
      ],
      "metadata": {
        "id": "nKJoY5asORZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System and Regex\n",
        "Let's define a simple system prompt, since there is no tool call required for this demo we can be fairly straightforward."
      ],
      "metadata": {
        "id": "T7CvWtw9jfR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"You are an AI Assistant with document understanding via URLs. You may be provided with URLs, followed by their corresponding OCR.\""
      ],
      "metadata": {
        "id": "Mkmw1FyGQpl3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract the URLs, we will use regex to extract any URL pattern from the user query.\n",
        "\n",
        "*Note: We will assume there will only be PDF files for simplicity.*"
      ],
      "metadata": {
        "id": "35yYt9asjoIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_urls(text: str) -> list:\n",
        "    url_pattern = r'\\b((?:https?|ftp)://(?:www\\.)?[^\\s/$.?#].[^\\s]*)\\b'\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    return urls"
      ],
      "metadata": {
        "id": "vLMw8Z8fOT19"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "\n",
        "#### Example Prompts ( PDFs )\n",
        "- Could you summarize what this research paper talks about? https://arxiv.org/pdf/2410.07073\n",
        "- Explain this architecture: https://arxiv.org/abs/2401.04088"
      ],
      "metadata": {
        "id": "gsRD_4mJjz7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": system}]\n",
        "while True:\n",
        "    user_input = input(\"User > \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    # Extract URLs from the user input, assuming they are always PDFs\n",
        "    document_urls = extract_urls(user_input)\n",
        "    user_message_content = [{\"type\": \"text\", \"text\": user_input}]\n",
        "    for url in document_urls:\n",
        "        user_message_content.append({\"type\": \"document_url\", \"document_url\": url})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message_content})\n",
        "\n",
        "    # Send the messages to the model and get a response\n",
        "    response = client.chat.complete(\n",
        "        model=text_model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
        "\n",
        "    print(\"Assistant >\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "Eell9TZ7Oapq",
        "outputId": "cc4c4c00-8a52-46ef-edcd-794962aa68dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User > Could you summarize what this research paper talks about? https://arxiv.org/pdf/2410.07073\n",
            "Assistant > The research paper introduces Pixtral 12B, a 12-billion-parameter multimodal language model designed to understand both natural images and documents. The model is trained to perform high-level reasoning and is capable of multi-turn, multi-image conversations. Pixtral 12B uses a new vision encoder trained from scratch, allowing it to process images at their native resolution and aspect ratio. This flexibility enables the model to handle images in latency-constrained settings or when fine-grained reasoning is required.\n",
            "\n",
            "Key points from the paper:\n",
            "\n",
            "1. **Architecture and Training**:\n",
            "   - Pixtral 12B is based on the transformer architecture and consists of a multimodal decoder and a vision encoder.\n",
            "   - The vision encoder, named PixtralViT, is designed to process images at variable resolutions and aspect ratios using a 400 million parameter vision transformer.\n",
            "   - The model uses RoPE-2D (Rotary Position Encoding 2D) to handle variable image sizes efficiently.\n",
            "\n",
            "2. **Performance**:\n",
            "   - Pixtral 12B outperforms other open models of similar sizes, such as Qwen-2-VL 7B and Llama-3.2 11B, on various multimodal benchmarks like MMMU, MathVista, and ChartQA.\n",
            "   - It also surpasses much larger models like Llama-3.2 90B on multimodal tasks.\n",
            "   - The model maintains strong performance on text-only benchmarks, making it a versatile tool for both text and vision tasks.\n",
            "\n",
            "3. **Evaluation and Benchmarks**:\n",
            "   - The paper introduces MM-MT-Bench, a new benchmark for evaluating multimodal instruction-following capabilities.\n",
            "   - The evaluation protocol includes 'Explicit' prompts that specify the output format, ensuring fair and standardized evaluation.\n",
            "   - The model's performance is robust across different parsing constraints, demonstrating its ability to follow instructions accurately.\n",
            "\n",
            "4. **Applications**:\n",
            "   - Pixtral 12B can be used for various applications, including reasoning over complex figures, multi-image instruction following, chart understanding, and converting images to code.\n",
            "   - The model's capabilities are demonstrated through qualitative examples, showing its effectiveness in real-world scenarios.\n",
            "\n",
            "5. **Contributions**:\n",
            "   - The paper contributes an open-source benchmark, MM-MT-Bench, and provides detailed analysis and code for standardized evaluation protocols for multimodal LLMs.\n",
            "   - Pixtral 12B is released under the Apache 2.0 license, making it accessible for further research and development.\n",
            "\n",
            "In summary, Pixtral 12B is a state-of-the-art multimodal model that excels in both text-only and multimodal tasks, offering flexibility and high performance across various benchmarks.\n",
            "User > Explain this architecture: https://arxiv.org/abs/2401.04088\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SDKError",
          "evalue": "API error occurred: Status 400\n{\"object\":\"error\",\"message\":\"Invalid document type. text/html is not supported.\",\"type\":\"invalid_file\",\"param\":null,\"code\":\"1901\"}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fdf518239077>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Send the messages to the model and get a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     response = client.chat.complete(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mistralai/chat.py\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, prediction, safe_prompt, retries, server_url, timeout_ms, http_headers)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4XX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mhttp_res_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             raise models.SDKError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"API error occurred\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_res_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             )\n",
            "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 400\n{\"object\":\"error\",\"message\":\"Invalid document type. text/html is not supported.\",\"type\":\"invalid_file\",\"param\":null,\"code\":\"1901\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icc3lutxAgNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}